{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Hotel Reviews\n",
    "\n",
    "This notebook loads customer hotel reviews, classifies their sentiment using `analyze_sentiment` (GPT-4o),\n",
    "and compares the predicted sentiment with the original `survey_sentiment` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add scripts/ to path so we can import the recommender module\n",
    "NOTEBOOK_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"scripts\"))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "recommender = importlib.import_module(\"ner-trip-recommender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- `customer_surveys_hotels_1k.json` — contains `id`, `review`, `customer_satisfaction_score`, `survey_sentiment`\n",
    "- `customer_surveys_hotels_1k_ner.json` — contains `id`, `text` (same review), `entities`\n",
    "\n",
    "We join both on `id` to get the full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "with open(DATA_DIR / \"customer_surveys_hotels_1k.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    surveys = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / \"customer_surveys_hotels_1k_ner.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    surveys_ner = json.load(f)\n",
    "\n",
    "# Build lookup by id\n",
    "survey_by_id = {s[\"id\"]: s for s in surveys}\n",
    "\n",
    "print(f\"Loaded {len(surveys)} surveys and {len(surveys_ner)} NER records\")\n",
    "print(f\"Sample keys (surveys): {list(surveys[0].keys())}\")\n",
    "print(f\"Sample keys (NER):     {list(surveys_ner[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify sentiment\n",
    "\n",
    "For each review we call `analyze_sentiment` which returns:\n",
    "```json\n",
    "{\"sentiment_score\": 1-5, \"reasoning\": \"...\"}\n",
    "```\n",
    "\n",
    "We map the 1-5 score to sentiment categories:\n",
    "- **1-2** → `\"negative\"`\n",
    "- **3** → `\"neutral\"`\n",
    "- **4-5** → `\"positive\"`\n",
    "\n",
    "This matches the 3-class `survey_sentiment` labels (positive/negative/neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, record in enumerate(surveys_ner):\n",
    "    review_id = record[\"id\"]\n",
    "    review_text = record[\"text\"]\n",
    "    survey = survey_by_id.get(review_id, {})\n",
    "\n",
    "    sentiment = recommender.analyze_sentiment(review_text)\n",
    "    \n",
    "    # Convert 1-5 sentiment score to category\n",
    "    score = sentiment[\"sentiment_score\"]\n",
    "    if score <= 2:\n",
    "        predicted = \"negative\"\n",
    "    elif score == 3:\n",
    "        predicted = \"neutral\"\n",
    "    else:  # score >= 4\n",
    "        predicted = \"positive\"\n",
    "\n",
    "    results.append({\n",
    "        \"id\": review_id,\n",
    "        \"review\": review_text,\n",
    "        \"customer_satisfaction_score\": survey.get(\"customer_satisfaction_score\"),\n",
    "        \"survey_sentiment\": survey.get(\"survey_sentiment\"),\n",
    "        \"predicted_sentiment_score\": score,\n",
    "        \"predicted_sentiment\": predicted,\n",
    "    })\n",
    "\n",
    "    if (i + 1) % 50 == 0 or i == 0:\n",
    "        print(f\"[{i+1}/{len(surveys_ner)}] id={review_id[:8]}... survey={survey.get('survey_sentiment')} predicted={predicted} (score={score})\")\n",
    "\n",
    "print(f\"\\nDone. Classified {len(results)} reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = DATA_DIR / \"sentiment_classification_results.json\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(results)} records to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load results from file (use this cell to skip the classification step above)\n",
    "with open(DATA_DIR / \"sentiment_classification_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} results from sentiment_classification_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "survey_counts = Counter(r[\"survey_sentiment\"] for r in results)\n",
    "predicted_counts = Counter(r[\"predicted_sentiment\"] for r in results)\n",
    "score_dist = Counter(r[\"predicted_sentiment_score\"] for r in results)\n",
    "\n",
    "match = sum(1 for r in results if r[\"survey_sentiment\"] == r[\"predicted_sentiment\"])\n",
    "total = len(results)\n",
    "\n",
    "print(\"Survey sentiment distribution (3-class):\")\n",
    "for label, count in survey_counts.most_common():\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "print(f\"\\nPredicted sentiment distribution (3-class):\")\n",
    "for label, count in predicted_counts.most_common():\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "print(f\"\\nPredicted score distribution (1-5):\")\n",
    "for score in sorted(score_dist.keys()):\n",
    "    print(f\"  {score}: {score_dist[score]}\")\n",
    "\n",
    "print(f\"\\nExact match (survey == predicted): {match}/{total} ({match/total*100:.1f}%)\")\n",
    "print(f\"\\nBoth survey_sentiment and predicted_sentiment now use 3 classes:\")\n",
    "print(f\"  - negative (score 1-2)\")\n",
    "print(f\"  - neutral (score 3)\")\n",
    "print(f\"  - positive (score 4-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (3-class classification)\n",
    "\n",
    "The model now predicts sentiment on a 1-5 scale, which is converted to 3 classes (positive/negative/neutral).\n",
    "This matches the survey data's 3-class labels, allowing for direct comparison across all 1000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# 3-class evaluation (all 1000 samples)\n",
    "y_true = [r[\"survey_sentiment\"] for r in results]\n",
    "y_pred = [r[\"predicted_sentiment\"] for r in results]\n",
    "\n",
    "print(f\"Samples: {len(y_true)} (all reviews included)\\n\")\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Overall Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Per-class metrics (macro average treats all classes equally)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Macro Precision: {precision_macro:.3f}\")\n",
    "print(f\"Macro Recall:    {recall_macro:.3f}\")\n",
    "print(f\"Macro F1-score:  {f1_macro:.3f}\")\n",
    "\n",
    "print(f\"\\nFull classification report (3-class):\")\n",
    "print(classification_report(y_true, y_pred, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix (3-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "# 3-class confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "cm_text = [[str(val) for val in row] for row in cm]\n",
    "\n",
    "fig_cm = ff.create_annotated_heatmap(\n",
    "    z=cm,\n",
    "    x=[f\"pred: {l}\" for l in labels],\n",
    "    y=[f\"true: {l}\" for l in labels],\n",
    "    annotation_text=cm_text,\n",
    "    colorscale=\"Blues\",\n",
    "    showscale=True,\n",
    ")\n",
    "fig_cm.update_layout(\n",
    "    title=\"Confusion Matrix (3-class: negative/neutral/positive)\",\n",
    "    xaxis_title=\"Predicted\",\n",
    "    yaxis_title=\"Actual (Survey)\",\n",
    "    height=500,\n",
    "    width=600,\n",
    ")\n",
    "fig_cm.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
